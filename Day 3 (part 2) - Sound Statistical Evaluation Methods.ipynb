{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sound Statistical Evaluation Methods\n",
    "\n",
    "#### In supervised learning we want a sound method to determine how well our classifier is working. There are three stand methods used to make this determination.\n",
    "\n",
    "### Data Splitting\n",
    "#### If you have a lot of labeled data say more than 10,000 observations the data splitting is fine. Common data splits are 80/20 or 90/10. \n",
    "\n",
    "### Cross-Validation\n",
    "#### When you have less data, say < 10,000 observations 10-fold cross-validation is an accepted method to evaluate the performance of your classifier. In 10-fold cross-validation we split our data into 10 equal folds and we hold out each fold for testing and we train with the other 9-folds. So essentially we are training with 90% of our data and testing with 10%. We iterate over each of the 10 folds, holding out a different fold for testing and training with the remaining 9-folds. We will do this 10 times and obtain 10 evaluation scores which we will average to determine the performance of our classifier.\n",
    "\n",
    "### Bootstrapping\n",
    "#### When we are dealing with small datasets < 1000 observations bootstrapping can be used to obtain an accurate picture of our classifier's performance characteristics. The benefit of bootstrapping is that we do not have to reduce our training set beyond the original number of observations that we have. We randomly sample with replacement from our original observation set for the number of observations that we started with. For example if we have 100 observations we sample with replacement 100 observations from our original data set. This training set is referred to as our \"in-the-bag\" (ITB) training set. The unselected cases in our original data set are then used for testing. This set is referred to as our \"out-of-bag\" (OOB) test set. We typically iterate on this process at least 2000 times and average the results to determine the final performance score. Surprisingly, and mathematically provable, the OOB test set with be made up of ~33% or our original data set and our ITB data set will be made up of 67% distant observations with 33% duplicates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold Cross-Validation\n",
    "\n",
    "#### Note that each test set is a unique set of observations from our original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold = 1\n",
    "kf = KFold(n=100, n_folds=10, shuffle=True)\n",
    "for X_train, X_test in kf:\n",
    "    print (\"Fold - \" + str(fold))\n",
    "    print(\"Training Set: \" + str(X_train))\n",
    "    print(\"Test Set \" + str(X_test))\n",
    "    print()\n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "#### Note that each training set is the same size as our original data set and that our test set is a set of distinct cases from our original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.arange(100);\n",
    "n = len(X)\n",
    "OOBAverage = 0;\n",
    "for i in range(1, 100):\n",
    "    print(\"Iteration - \" + str(i))\n",
    "    ITB = np.random.choice(n, n, replace=True)\n",
    "    X_ITB = X[ITB]\n",
    "    print(\"Training Set (%d): %s\" % (len(X_ITB), str(X_ITB)))\n",
    "    X_OOB = np.delete(X, list(set(ITB)), 0)\n",
    "    print(\"Test Set (%d): %s\" % (len(X_OOB), str(X_OOB)))\n",
    "    print()\n",
    "    OOBAverage += len(X_OOB)\n",
    "print()\n",
    "print(\"Average OOB:\" + str(OOBAverage/i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
