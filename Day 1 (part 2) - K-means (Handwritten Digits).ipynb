{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis with K-means (MNIST Handwritten Digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Clustering\n",
    "#### Clustering is an unsupervised learning problem where we group data according to some similarity hidden within the data. Typically in clustering we have data without labels where the data is represented as a matrix $X \\in \\mathbb{R}^{NxD}$ over $N$ observations and $D$ dimensions or features. K-means is a clustering or unsupervised learning method where we want to break $N$ observations into $K$ groups. This form of clustering is also known as Flat Clustering.\n",
    "\n",
    "#### In K-means we want to select groups where . . .\n",
    "\n",
    "#### 1) High intercluster similarity: $\\sum_{k}\\sum_{n \\neq m\\in k} d(x_n,x_m)$ should be low\n",
    "\n",
    "#### 2) Low intracluster simularity: $\\sum_{k \\neq k'}\\sum_{n \\in k}\\sum_{m \\in k'} d(x_n,x_m)$ should be high\n",
    "\n",
    "#### where $d(x,mu) = \\Bigl(\\sqrt{\\sum_{k=1}^n\\mid(x_k - mu_k)\\mid^2}\\Bigr)^2$\n",
    "\n",
    "\n",
    "#### K-means operates in high-diemnsional space  $\\mathbb{R}^{NxD}$ so we need a method to measure distance between two points in this space ($Euclidian \\space space$) known as the $Euclidian \\space norm$ or $L^2 norm$. Mathematically, a norm is a total size or length of all vectors in a vector space or matrices.  The $L^2 norm$ is used as a standard quantity for measuring a vector difference. We will see this later. In  $X \\in \\mathbb{R}^{2}$ it looks something like this.\n",
    "\n",
    "\n",
    "![title](Euclidian.png)\n",
    "\n",
    "\n",
    "#### The idea behind the K-means algorithm is to propose a defined number of clusters $K$ and then iteratively fix the errors to optimize the criteria discussed above. \n",
    "\n",
    "####            $\\bullet$ Initialize cluster centers\n",
    "####            $\\bullet$ Assign each data point to the closeest center\n",
    "####            $\\bullet$ Recompute the centers as the means of the assigned data points\n",
    "####            $\\bullet$ If the assignments have changed, go to (2)\n",
    "\n",
    "\n",
    "#### This is the k-means algorithm. It is very sensative to initialization, but converges quickly.\n",
    "#### There are a couple different ways to initialize the cluster centers also called the centroids. \n",
    "\n",
    "####            $\\bullet$ Random points drawn from a \"reasonable\" input space. What's reasonable?\n",
    "####            $\\bullet$ First select a random data point, then a second data point as far away as possible, then a third data point as far away as possible, and so on, and so on....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "#### We will try and cluster some handwritten digits from the MINST database.\n",
    "\n",
    "#### First we will import some Python packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import numpy.matlib as ml\n",
    "from scipy import ndimage\n",
    "from itertools import repeat\n",
    "from numpy import linalg as la\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the K-means Algorithm\n",
    "\n",
    "#### We will first need a function to calcualte the minimum distance between a set of vector features. See if you can figure out what is going on here .... Do you see where the $L^2 norm$ is being calculated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minDistance(X, C):\n",
    "    len, dim = C.shape\n",
    "    D = ml.zeros((len, 1), 'float64')\n",
    "    for i in range(0, len):\n",
    "        D[i] = la.norm(X - C[i,:],2)**2\n",
    "    k = D.argmin()\n",
    "    dist = min(D)[0,0]\n",
    "    return k, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next we will define the k-means algorithm that we described above. Note that we have implemented both random initialization and furthest centroid distance initialization. Read the comments in the code to try and figure out what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kmeans(X, K, init):\n",
    "    N, D  = X.shape\n",
    "    if (K >= N):\n",
    "        raise Exception('kmeans: you are trying to make too many clusters!');\n",
    "    if (init == 'random'):\n",
    "        # initialize by choosing K (distinct!) random points: we do this by\n",
    "        # randomly permuting the examples and then selecting the first K points\n",
    "        perm = np.random.permutation(N);\n",
    "        perm = perm[:K]\n",
    "        # the centers are given by selected vectors\n",
    "        mu = X[perm, :]\n",
    "        # we leave the assignments unspecified -- they'll be computed in the\n",
    "        # first iteration of K means\n",
    "        z = ml.zeros((N, 1), 'float64')\n",
    "    elif (init == 'furthest'):\n",
    "        # Randomly choose the first center c_1.  \n",
    "        # then, let c_2 = argmax dist(x_i, c_1).\n",
    "        # let  c_3 = argmax dist [ dist(x_i, c_1), dist(x_i, c_2) ] \n",
    "        # and so on....  \n",
    "        perm = np.random.permutation(N);\n",
    "        mu = X[perm[0], :].reshape(1, D)\n",
    "        XT = np.delete(X, perm[0], 0)\n",
    "        for i in range(1,K):\n",
    "            len, dim = XT.shape\n",
    "            DX = ml.zeros((len,1), 'float64')\n",
    "            for j in range(0, len):\n",
    "                k, dist = minDistance(XT[j,:].reshape(1, dim), mu);\n",
    "                DX[j] = dist;\n",
    "            indc = DX.argmax()\n",
    "            r, c = mu.shape\n",
    "            mu = np.append(mu, XT[indc,:].reshape(1, dim))\n",
    "            mu = mu.reshape((r + 1), c)\n",
    "            XT = np.delete(XT, indc, 0)          \n",
    "        # again, don't bother initializing z\n",
    "        z = ml.zeros((N, 1), 'float64')\n",
    "    else:\n",
    "        raise Exception('unknown initialization: use \"furthest\" or \"random\"');\n",
    "    # begin the iterations.  we'll run for a maximum of 20, even though we\n",
    "    # know that things will *eventually* converge.\n",
    "    for itr in range(0, 20):\n",
    "        # in the first step, we do assignments: each point is assigned to the\n",
    "        # closest center.  we'll judge convergence based on these assignments,\n",
    "        # so we want to keep track of the previous assignment\n",
    "        oldz = np.array(z);\n",
    "        for n in range(0,N):\n",
    "            # assign point n to the closest center\n",
    "            k, dist = minDistance(X[n,:].reshape(1, D), mu);\n",
    "            z[n] = k;  \n",
    "            # check to see if we've converged\n",
    "        if all(oldz==z):\n",
    "            break;\n",
    "        # re-estimate the means            \n",
    "        for k in range(0,K):\n",
    "            mu[k,:] = X[np.where(z == k)[0], :].mean(0).reshape(1, D)             \n",
    "        # final: compute the score\n",
    "    score = 0;\n",
    "    for n in range(0, N):\n",
    "        # compute the distance between X(n,:) and it's associated mean\n",
    "        score = score + la.norm(X[n,:] - mu[int(z[n][0,0]),:])**2;\n",
    "    return (mu, z, score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in our Dataset\n",
    "\n",
    "#### Let's read in our MNIST handwritten digit images. We are going to cheat a little, we are borrowing this data set from MATLAB which gives us pre-populated matricies to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist = sio.loadmat('mnist.mat')\n",
    "testX = mnist['testX']\n",
    "testY = mnist['testY']\n",
    "trainX = mnist['trainX']\n",
    "trainY = mnist['trainY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Clusters\n",
    "\n",
    "#### We will analyze our handwritten digits by evaluating diffent cluster sets. We will look at $K \\in \\{2, 5, 10, 15, 20\\}$. \n",
    "\n",
    "#### 1) What do you think the score is being produced? $You{\\space}should{\\space}know.$ \n",
    "#### 2) Is a higher score or lower score better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = trainX\n",
    "Y = trainY\n",
    "allK = [2, 5, 10, 15, 20]\n",
    "scores = list(repeat(0, len(allK)))\n",
    "zs = []\n",
    "mus = []\n",
    "\n",
    "for ii in range(0, len(allK)):\n",
    "    print(\"K = %d...\" % allK[ii], end=\"\", flush=True)\n",
    "    bestScore = float(\"inf\");\n",
    "    bestZ     = 0;\n",
    "    bestMu    = 0;\n",
    "    for rep in range(1,3):\n",
    "        mu,z,s = kmeans(X, allK[ii],'furthest');\n",
    "        if (s < bestScore):\n",
    "            bestScore = s;\n",
    "            bestZ = z;\n",
    "            bestMu = mu;\n",
    "    print(\" --> score %g\\n\" % bestScore) \n",
    "    scores[ii] = bestScore\n",
    "    zs.append(bestZ)\n",
    "    mus.append(bestMu)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Metrics for Evaluation\n",
    "\n",
    "#### We need to calculate some statistical measures to evaluate our clusters. We will display a graph of the regularlzed score with $AIC$ and $BIC$. All of these scoures are normalized so that they take a maximum value of 1 so they fit on the same plot. \n",
    "\n",
    "#### 1) What does $BIC$ and $AIC$ tell us? Can you figure out if a higher or lower score is better? \n",
    "#### 2) What are the trends of the plots as K increases? \n",
    "#### 3) Based on each score, BIC and AIC, what value of K would you choose to use?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bic = list(repeat(0, len(allK)))\n",
    "aic = list(repeat(0, len(allK)))\n",
    "for ii in range(0, len(allK)):\n",
    "    N = len(Y)\n",
    "    bic[ii] = N * np.log(scores[ii] / N) + allK[ii] * np.log(N);\n",
    "    aic[ii] = N * (np.log(2*math.pi*scores[ii] / N) + 1) + 2 * allK[ii];\n",
    "bic = bic / max(bic)\n",
    "aic = aic / max(aic)\n",
    "scores = list(scores / max(scores))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphing the Scores\n",
    "\n",
    "#### Can you answer the questions above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(\"Figure 1\")\n",
    "plt.title('K versus score for digits data')\n",
    "plt.plot(allK, scores, linestyle='-', marker='o', color='b')\n",
    "plt.plot(allK, aic, linestyle='-', marker='x', color='g')\n",
    "plt.plot(allK, bic, linestyle='-', marker='s', color='r')\n",
    "plt.ylim(ymax = 1.0, ymin = 0.6)\n",
    "plt.legend(['normalized score', 'normalized BIC', 'normalized AIC'], loc='lower left')\n",
    "plt.ylabel('score')\n",
    "plt.xlabel('K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Cluster Centroids\n",
    "\n",
    "#### Here is some code that will allow us to visualize the cluster centroids. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_digits(X,Y):\n",
    "    N, D = X.shape\n",
    "    DY = math.floor(math.sqrt(N));\n",
    "    DX = math.ceil(N / DY);\n",
    "    for n in range(0, N):\n",
    "        plt.suptitle(\"K = \" + str(N) + \" Clusters\")\n",
    "        plt.subplot(DY, DX, n+1)\n",
    "        Z = ml.uint8(255*X[n,:].reshape(28,28).T)\n",
    "        im = plt.imshow(ndimage.rotate(Z, 90), interpolation='bilinear', origin='lower')\n",
    "        plt.axis('off')    \n",
    "        plt.title('C = ' + str(Y[n]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization of our Handwritten Digit Clusters\n",
    "\n",
    "#### 1) Take a look, what do you think -  do the clusters seem reasonable? \n",
    "#### 2) For each of the figures, how many \"real\" digits can you recognize?  Which ones are missing? Which ones seem to have been merged?\n",
    "#### 3) What seems to be the best number of clusters considering all the graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(\"Figure 2\")\n",
    "draw_digits(mus[0], list(range(1, allK[0]+1)))\n",
    "plt.figure(\"Figure 3\")\n",
    "draw_digits(mus[1], list(range(1, allK[1]+1)))\n",
    "plt.figure(\"Figure 4\")\n",
    "draw_digits(mus[2], list(range(1, allK[2]+1)))\n",
    "plt.figure(\"Figure 5\")\n",
    "draw_digits(mus[3], list(range(1, allK[3]+1)))\n",
    "plt.figure(\"Figure 6\")\n",
    "draw_digits(mus[4], list(range(1, allK[4]+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
