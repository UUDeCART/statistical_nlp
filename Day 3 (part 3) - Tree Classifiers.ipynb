{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Classifiers\n",
    "\n",
    "### Decision Tree Construction Algorithm\n",
    "\n",
    "#### We build a tree top-down in a greedy manner.\n",
    "\n",
    "    function BuildDT(X, Y, threshold)\n",
    "        if (all the labels (Y) are the same\n",
    "            assign as leaf node with label set to class of Y\n",
    "            return tree\n",
    "        elseif (threshold is the minimum number of examples to keep)\n",
    "            assign as leaf node with label set to most commin class\n",
    "            return tree\n",
    "        else\n",
    "            let f be the feature that is best to split on\n",
    "            let leftChildBranch = BuildDT(data (X) where f=True, labels (Y) with f=True)\n",
    "            let rightChildBranch = BuildDT(data (X) where f=False, labels (Y) with f=False)\n",
    "            return tree\n",
    "            \n",
    "### Determining best split feature by evaluating node purity\n",
    "\n",
    "<img style=\"float: l;\" src=\"Purity.png\">\n",
    " \n",
    "$Entropy(t) = - \\displaystyle \\sum_{i=0}^{c-1} p(\\space i \\space | \\space t \\space) \\space log_2 \\space p(\\space i \\space | \\space t \\space) \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space Gini(t) = 1 - \\displaystyle \\sum_{i=0}^{c-1} [p(\\space i \\space | \\space t \\space)]^2 \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space Classification error(t) = 1 - \\max_i [p(\\space i \\space | \\space t \\space)]$\n",
    "\n",
    "#### 1) Determine which feature is better to split on ?\n",
    "<img style=\"float: left;\" src=\"Gini.png\">  $ A.N1 = 1 - (4/7)^2 + (3/7)^2 = 0.4898 \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space$\n",
    "                                           $ A.N2 = 1 - (2/5)^2 + (3/5)^2 = 0.4800 \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space \\space$\n",
    "                                           $A = (7/12) x 0.4898 + (5/12) X 0.4800 = 0.486$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting Weak Classifiers (Decision Stumps)\n",
    "\n",
    "#### AdaBoost is a method by which we take a weak classifier and construct a distribution over the input data observations. The key is to constantly change the distribution of data on which we train our “weak learner” so that it learns to correct its own mistakes. It runs in $T$ rounds and in each round learns a weak hypothesis $h_t$. It also learns a $coefficient \\space \\alpha_t$ for each weak hypothesis. The final prediction for an input $x$ is:\n",
    "\n",
    "$\\hat{y} = sign \\big{[} \\displaystyle \\sum_{t=1}^T \\alpha_t h_t (\\hat{x}) \\big{]} $\n",
    "\n",
    "#### We define a classification algorithm  (the weak learner) $A \\big{(} \\big{\\langle} x_n, y_n, D_n \\big{\\rangle}_{n=1}^N \\big{)} $ that takes as input the training data, training labels, and a weighting vector for each observation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                           \n",
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_curve\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic NLP Processing\n",
    "\n",
    "#### We will define some Python functions that will perform some syntatic work on our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [ token for token in tokens if re.search('(^[a-zA-Z]+$)', token) ]\n",
    "    return filtered_tokens\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\") + ['year', 'old', 'man', 'woman', 'ap', 'am', 'pm', 'portable', 'pa', 'lat', 'admitting', 'diagnosis', 'lateral']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving our Corpus\n",
    "\n",
    "#### Let's pull in our corpus that we had serialized out to disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('classification-corpus.pkl','rb')\n",
    "corpus = pkl.load(file)\n",
    "file.close()\n",
    "corpusList = list(corpus.values())\n",
    "labels = list(corpus.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Document-Term Frequency Counts\n",
    "\n",
    "#### In this step we tokenize our text and remove stop words in addition to generating our frequency counts.\n",
    "\n",
    "#### 1) How many documents are we working with and how many features (unigrams & bigrams)?\n",
    "\n",
    "#### 2) Can you figure out what max_df and min_df is doing to our feature count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True, max_df=0.80, max_features=None, min_df=0.033,\n",
    "                     ngram_range=(1, 2), preprocessor=None, stop_words=cachedStopWords,\n",
    "                     strip_accents=None, tokenizer=tokenize, vocabulary=None)\n",
    "X = cv.fit_transform(corpusList)\n",
    "print(X.shape)\n",
    "print()\n",
    "lexicon = cv.get_feature_names()\n",
    "print (lexicon)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct our Classes\n",
    "\n",
    "#### We need to assign a class for each classification. We typically assign numeric values to classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = []\n",
    "for key in corpus:        \n",
    "    if (key.startswith('Trauma')):\n",
    "        Y.append(0)\n",
    "    elif (key.startswith('PNA')):\n",
    "        Y.append(1)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Run It!\n",
    "\n",
    "#### We will generate models and evaluate the modes using bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "truth = []\n",
    "dt_prediction = []\n",
    "bdt_prediction = []\n",
    "rf_prediction = []\n",
    "\n",
    "for i in range(0,10):\n",
    "    print('Interation: ' + str(i+1))\n",
    "    N, D = X.shape\n",
    "    ITB = np.random.choice(N, N, replace=True)\n",
    "    X_ITB = X[ITB, :]\n",
    "    Y_ITB = Y[ITB]\n",
    "    X_OOB = np.delete(X.A, list(set(ITB)), 0)\n",
    "    Y_OOB = np.delete(Y, list(set(ITB)), 0)\n",
    "    N_OOB, D_OOB = X_OOB.shape\n",
    "    truth.append(Y_OOB)\n",
    "    dt = DecisionTreeClassifier(random_state=0)\n",
    "    dt.fit(X_ITB, Y_ITB)\n",
    "    Y_hat = dt.predict(X_OOB)\n",
    "    dt_prediction.append(Y_hat)\n",
    "    bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=201)\n",
    "    bdt.fit(X_ITB, Y_ITB)\n",
    "    Y_hat = bdt.predict(X_OOB)\n",
    "    bdt_prediction.append(Y_hat)\n",
    "    rf = RandomForestClassifier(n_estimators = 101, oob_score = True, n_jobs = -1, max_features = \"auto\")\n",
    "    rf.fit(X_ITB, Y_ITB)\n",
    "    Y_hat = rf.predict(X_OOB)\n",
    "    rf_prediction.append(Y_hat)\n",
    "\n",
    "truth = np.concatenate(truth, axis=0)    \n",
    "dt_prediction = np.concatenate(dt_prediction, axis=0)\n",
    "bdt_prediction = np.concatenate(bdt_prediction, axis=0)\n",
    "rf_prediction = np.concatenate(rf_prediction, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contingency Tables\n",
    "\n",
    "#### Let's look at the contingency tables\n",
    "\n",
    "#### 1) Can you calculate the Sensitivity, Specificity, PPV, NPV? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dt_ct = pd.crosstab(dt_prediction, truth, margins=True)\n",
    "dt_ct.columns = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "dt_ct.index = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "print(\"Decision Tree\")\n",
    "print(dt_ct)\n",
    "print()\n",
    "\n",
    "bdt_ct = pd.crosstab(bdt_prediction, truth, margins=True)\n",
    "bdt_ct.columns = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "bdt_ct.index = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "print(\"Boosting Decision Stumps\")\n",
    "print(bdt_ct)\n",
    "print()\n",
    "\n",
    "rf_ct = pd.crosstab(rf_prediction, truth, margins=True)\n",
    "rf_ct.columns = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "rf_ct.index = [\"Trauma\", \"PNA\", \"Total\"]\n",
    "print(\"Random Forest\")\n",
    "print(rf_ct)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Statistics\n",
    "\n",
    "#### We will calculate some test statistics for our classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Sens = dt_ct.iloc[1][1]/dt_ct.iloc[2][1]\n",
    "Spec = dt_ct.iloc[0][0]/dt_ct.iloc[2][0]\n",
    "PPV = dt_ct.iloc[1][1]/dt_ct.iloc[1][2]\n",
    "NPV = dt_ct.iloc[0][0]/dt_ct.iloc[0][2]\n",
    "ACC = (dt_ct.iloc[0][0] + dt_ct.iloc[1][1]) / dt_ct.iloc[2][2]\n",
    "print(\"Decision Tree: Sensitivity: %.5f Specificity: %.5f PPV: %.5f NPV: %.5f Accuracy: %.5f\" % (Sens, Spec, PPV, NPV, ACC))\n",
    "\n",
    "Sens = bdt_ct.iloc[1][1]/bdt_ct.iloc[2][1]\n",
    "Spec = bdt_ct.iloc[0][0]/bdt_ct.iloc[2][0]\n",
    "PPV = bdt_ct.iloc[1][1]/bdt_ct.iloc[1][2]\n",
    "NPV = bdt_ct.iloc[0][0]/bdt_ct.iloc[0][2]\n",
    "ACC = (bdt_ct.iloc[0][0] + bdt_ct.iloc[1][1]) / bdt_ct.iloc[2][2]\n",
    "print(\"Boosting Decision Stumps: Sensitivity: %.5f Specificity: %.5f PPV: %.5f NPV: %.5f Accuracy: %.5f\" % (Sens, Spec, PPV, NPV, ACC))\n",
    "\n",
    "Sens = rf_ct.iloc[1][1]/rf_ct.iloc[2][1]\n",
    "Spec = rf_ct.iloc[0][0]/rf_ct.iloc[2][0]\n",
    "PPV = rf_ct.iloc[1][1]/rf_ct.iloc[1][2]\n",
    "NPV = rf_ct.iloc[0][0]/rf_ct.iloc[0][2]\n",
    "ACC = (rf_ct.iloc[0][0] + rf_ct.iloc[1][1]) / rf_ct.iloc[2][2]\n",
    "print(\"Random Forest: Sensitivity: %.5f Specificity: %.5f PPV: %.5f NPV: %.5f Accuracy: %.5f\" % (Sens, Spec, PPV, NPV, ACC))                                                                                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ROC Curve\n",
    "\n",
    "#### 1) So which classifier do you think is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dt_fpr, dt_tpr, dt_thresholds = roc_curve(truth, dt_prediction, pos_label=1)\n",
    "bdt_fpr, bdt_tpr, bdt_thresholds = roc_curve(truth, bdt_prediction, pos_label=1)\n",
    "rf_fpr, rf_tpr, rf_thresholds = roc_curve(truth, rf_prediction, pos_label=1)\n",
    "plt.figure(1)\n",
    "#plt.xlim(0, 0.2)\n",
    "#plt.ylim(0.8, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(dt_fpr, dt_tpr, label='DecisionTree')\n",
    "plt.plot(bdt_fpr, bdt_tpr, label='BoostingDecisionStumps')\n",
    "plt.plot(rf_fpr, rf_tpr, label='RandomForest')\n",
    "plt.xlabel('False positive rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
