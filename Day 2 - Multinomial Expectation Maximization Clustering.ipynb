{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Analysis with Multinomial EM (Text Clustering)\n",
    "\n",
    "#### In the K-means clustering algorithm we forced documents to be assigned to one cluster. This is known as hard clustering. This approach in not really accurate when working with language.  When someone writes about a topic they use a certain terminology to describe that topic. We assume that when someone else writes about the same topic they are likely to use the same terminology or words. But words may also be used across multiple topics and a clinical note may describe multiple topics. It may be more appropriate to assign words and documents to multiple topics (or clusters) with a certain probability based on their use.\n",
    "\n",
    "#### This example illustrates how to group a set of different clinical notes based on their topics and the terminology used to describe the topics. We will model our clinical notes as a collection of unigram and bigram words. Specifically, we will represent each clinical note as a feature set of unigram and bigram frequencies found in the clinical note. We will use a matrix where each row will represent a clinical note and each column a feature (i.e. distinct unigram or bigram).\n",
    "\n",
    "### Mixture of Multinomial Distributions\n",
    "\n",
    "#### Text is best represented as a mixture of multinomial distributions where each topic has a particular multinomial distribution associated with it and each document in a mixture of topics. \n",
    "\n",
    "#### Formally, let $p(c)\\space=\\space\\pi_c$ be the prior probability of a document containing topic c, and each topic c is represented as a multinomial distribution $p(D_i|c)$ with parameters $\\mu_{jc}$, then each document becomes a mixture over topics as\n",
    "\n",
    "#### $p(D_i)\\space=\\space \\displaystyle\\sum_{c=1}^{n_c} p(D_i|c)p(c) = \\displaystyle\\sum_{c=1}^{n_c} \\pi_c \\prod_{j=1}^{n_w}\\mu_{jc}^{T_{ij}}$\n",
    "\n",
    "### Expectation Maximization for Mixtures of Multinomials\n",
    "\n",
    "#### The expectation maximization algorithm will allow us to fit a multinomial mixture model to our data. Our goal is to identify which documents belong to which topics and what words (unigrams and bigrams) are used to describe the topic. \n",
    "\n",
    "#### 1. E-Step. Compute the expectation that document $D_i$ belongs to topic (cluster) $c$\n",
    "\n",
    "####         $\\gamma_{ic} \\propto \\pi_c \\displaystyle\\prod_{j = 1}^{n_w} \\mu_{jc}^{T_{ij}}$\n",
    "\n",
    "#### $Note: \\space normalize \\space expectations \\space over \\space c$\n",
    "\n",
    "#### where,\n",
    "#### $\\pi_c \\equiv prior{\\space}probability{\\space}of{\\space}document{\\space}containing{\\space}topic{\\space}c$\n",
    "#### $\\mu_{jc} \\equiv probability{\\space}of{\\space}w_j{\\space}in{\\space}topic{\\space}c$\n",
    "#### $T_{ij} \\equiv count \\space of \\space w_j \\space in \\space topic \\space c$\n",
    "\n",
    "\n",
    "#### 2. M-Step. Update the mixture parameters. \n",
    "\n",
    "#### $\\mu_{jc} = \\frac{\\sum_{i = 1}^{n_d} \\gamma_{ic}T_{ij}}{\\sum_{i = 1}^{n_d}\\sum_{l = 1}^{n_w} \\gamma_{ic}T_{il}} \\equiv probability \\space of \\space a \\space word \\space being \\space w_j \\space in \\space topic \\space (cluster) \\space c$ \n",
    "\n",
    "#### $\\pi_c = \\frac{1}{n_d} \\displaystyle \\sum_{i = 1}^{n_d} \\gamma_{ic} \\equiv prior \\space probability \\space of \\space each \\space cluster$\n",
    "\n",
    "#### $Note: \\space normalize \\space priors \\space uniformly,\\space initialize \\space \\mu's \\space to \\space multinomial \\space generated \\space from \\space uniform \\space dirichlet \\space distribution \\space such \\space that \\sum_{j = 1}^{n_w} \\mu_{jc} = 1$\n",
    "\n",
    "### Implementation\n",
    "\n",
    "#### Our implementation will consist of miipulating 4 matricies as described below.\n",
    "\n",
    "![title](MultinomialEMClustering.png)\n",
    "\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "#### First we will import some Python packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import numpy.matlib as ml\n",
    "import pickle as pkl\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntactic NLP Processing\n",
    "\n",
    "#### We need a function to tokenize our text and remove noise like dates, ages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = [ token for token in tokens if re.search('(^[a-zA-Z]+$)', token) ]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving our Corpus\n",
    "\n",
    "#### Let's pull in our corpus that we had serialized out to disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('corpus.pkl','rb')\n",
    "corpus = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Syntactic Processing\n",
    "\n",
    "#### We will want to get ride of stop words that are essentially noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cachedStopWords = stopwords.words(\"english\") + ['year', 'old', 'man', 'woman', 'ap', 'am', 'pm', 'portable', 'pa', 'lat', 'admitting', 'diagnosis', 'lateral']\n",
    "print(cachedStopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Document-Term Frequency Counts\n",
    "\n",
    "#### In this step we tokenize our text and remove stop words in addition to generating our frequency counts.\n",
    "\n",
    "#### 1) how many documents are we working with and how many features (unigrams & bigrams)?\n",
    "\n",
    "#### 2) Can you figure out what max_df and min_df is doing to our feature count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpusList = list(corpus.values())\n",
    "labels = list(corpus.keys())\n",
    "rank = list(range(len(labels)))\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, max_df=0.80, max_features=None, min_df=0.033,\n",
    "                     ngram_range=(2, 2), preprocessor=None, stop_words=cachedStopWords,\n",
    "                     strip_accents=None, tokenizer=tokenize, vocabulary=None)\n",
    "SparseT = cv.fit_transform(corpusList)\n",
    "print(\"The dimensions of our document-term matrix\")\n",
    "print(SparseT.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Set\n",
    "\n",
    "#### Let's take a look at our feature set.\n",
    "\n",
    "#### 1) Do we have a lot of noise in our features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = cv.get_feature_names()\n",
    "print (lexicon)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define EM Algorithm (E-Step & M-Step)\n",
    "\n",
    "#### Now let's define our EM Algorithm.\n",
    "\n",
    "#### 1) In the ExpD function why are we multiplying by $1x10^2 \\space ?$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "def ExpD(T, Mu, Pi):    \n",
    "    C_n = Pi.shape[1]\n",
    "    D_n = T.shape[0]\n",
    "    Gamma = ml.zeros((D_n, C_n))\n",
    "    Gamma.astype('float64')\n",
    "    for c in range(0, C_n):\n",
    "        Gamma[:, c] = Pi[0][c] * ((Mu[:,c].A[:,0]*1e2)**T).prod(1)\n",
    "    Gamma = Gamma / Gamma.sum(axis=1)\n",
    "    return Gamma\n",
    "    \n",
    "def updateMu(T, Gamma):    \n",
    "    C_n = Gamma.shape[1]\n",
    "    W_n = T.shape[1]\n",
    "    Mu = ml.zeros((W_n, C_n))\n",
    "    for c in range(0, C_n):\n",
    "        numerator = sum(np.multiply(Gamma[:,c],T)).T\n",
    "        demoninator = sum(np.multiply(Gamma[:,c],T).sum(1))\n",
    "        Mu[:,c] = numerator / demoninator\n",
    "    return Mu\n",
    "    \n",
    "def updatePi(Gamma):    \n",
    "    D_n = Gamma.shape[0]\n",
    "    Pi = sum(Gamma) / D_n\n",
    "    return Pi.A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Run it !\n",
    "\n",
    "#### Let's start with 10 topics (clusters) and we will interate 100 times. EM converges quickly.\n",
    "\n",
    "#### 1) Can you determine at what iteration we are starting to reach convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = SparseT.todense()\n",
    "D_n, W_n = T.shape\n",
    "C_n = 10\n",
    "Pi = ml.repmat(1/C_n, 1, C_n)\n",
    "Mu = ml.mat(np.random.dirichlet(np.ones(W_n), C_n).T)\n",
    "for i in range(1,101):\n",
    "    print('Iteration: ' + str(i)) \n",
    "    Gamma = ExpD(T, Mu, Pi)\n",
    "    print(Gamma.sum(0))\n",
    "    Mu = updateMu(T, Gamma)\n",
    "    Pi = updatePi(Gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examination of Clusters and Terminology\n",
    "\n",
    "#### Let's take a look at the top cluster for each clinical note and the top 20 words to distinguish this topic.\n",
    "\n",
    "#### 1) Is there noise in the terminology? If there is how can we get ride of it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = Gamma.argmax(1).A\n",
    "clusters = [i[0] for i in clusters]\n",
    "clinicalDocuments = { 'labels': labels, 'rank': rank, 'corpus': corpusList, 'cluster': clusters }\n",
    "frame = pd.DataFrame(clinicalDocuments, index = clusters , columns = ['rank', 'labels', 'corpus', 'cluster'])\n",
    "grouped = frame['rank'].groupby(frame['cluster'])\n",
    "topWords = Mu.T.argsort()[:, ::-1]\n",
    "for i in range(C_n):\n",
    "    n = len(frame.ix[i]['labels'].values.tolist())\n",
    "    print('Cluster %d (%d):,' % (i+1, n), end='')\n",
    "    for label in frame.ix[i]['labels'].values.tolist():\n",
    "        print(' %s,' % label, end='')\n",
    "    print()\n",
    "    print()\n",
    "    print('           Words:', end='')\n",
    "    for indice in list(topWords[i, :20].A[0]):\n",
    "        print(' %s (%.5f)' % (lexicon[indice], Mu[indice,i]), end=',')\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Clustering Document Examination\n",
    "\n",
    "#### 1) Can you find clinical notes that belong to more than 1 cluster ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, C = Gamma.shape\n",
    "for i in range(N):\n",
    "    print('%s: ' % labels[i], end='')\n",
    "    for j in range(C):\n",
    "        print('C%d (%.7f): ' % (j+1, Gamma[i, j]), end='')\n",
    "    print()\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Clustering Term Examination\n",
    "\n",
    "#### 1) Can you find terms that belong to more than 1 cluster ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, C = Mu.shape\n",
    "for i in range(W):\n",
    "    print('%s: ' % lexicon[i], end='')\n",
    "    for j in range(C):\n",
    "        print('C%d (%.5f): ' % (j+1, Mu[i, j]), end='')\n",
    "    print()\n",
    "    print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
