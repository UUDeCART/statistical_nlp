{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster Analysis with K-means (Text Clustering)\n",
    "\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "#### First we will import some Python packages that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import MDS\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Environment Setup\n",
    "\n",
    "#### We may also need to pull in some nltk resources. These resources will help us clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntatic NLP Processing\n",
    "\n",
    "#### We will define some Python functions that will perform some syntatic work on our corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function will remove unnecessay puncuation that can add to noise\n",
    "def stripPunctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "# We will want to eventually tokenize a document into sentences and words\n",
    "def tokenize(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out tokens not containing letters (e.g., numeric tokens, some punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "# We cache the stopword upfront for perfomance\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "# We will want to remove stopwords which are typiccially high frequency words that aid with  \n",
    "# text fluency but do not provide much information gain.\n",
    "def removeStopwords(text):\n",
    "    no_stop = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving our Corpus\n",
    "\n",
    "#### Let's pull in our corpus that we had serialized out to disk.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('corpus.pkl','rb')\n",
    "corpus = pkl.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preparing our Corpus for Processing\n",
    "\n",
    "#### First step is to clean up our corpus. We will strip pucntuation, remove stopwords, and tokenize our text. Actually removing stopwords and tokinization can be done by a downstream function as you will see. We do it here on the first document in our corpus just so you can get a good sense how the corpus gets transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleanedCorpus = []\n",
    "ranks = []\n",
    "labels = []\n",
    "idx = 1\n",
    "for key in corpus:        \n",
    "    cleanedText = stripPunctuation(corpus[key])\n",
    "    if (idx < 2):\n",
    "        print(\"**** ORIGINAL CLINICAL DOCUMENT ****\")\n",
    "        print()\n",
    "        print(corpus[key])\n",
    "        print(\"**** SOME PUNCTUATION REMOVED ****\")\n",
    "        print()        \n",
    "        print(cleanedText)\n",
    "        print(\"**** STOP WORDS REMOVED ****\")\n",
    "        print()\n",
    "        textWithoutStopwords = removeStopwords(cleanedText)\n",
    "        print(textWithoutStopwords)\n",
    "        tokenizedText = tokenize(cleanedText)\n",
    "        print()\n",
    "        print(\"**** LOWER CASE AND TOKENIZE ****\")\n",
    "        print()\n",
    "        print(tokenizedText)\n",
    "        print()\n",
    "    cleanedCorpus.append(cleanedText)\n",
    "    labels.append(key)\n",
    "    ranks.append(idx)   \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Quantifying our Clinical Corpus for Cluster Analysis\n",
    "\n",
    "#### We need to somehow quantify the importance of the words found in a document relative to one another, as well as, relative to the entire corpus. Simply using frequency counts of words are a bias measurement. Longer documents naturally will have greater term frequencies than shorter documents. \n",
    "\n",
    "#### We can borrow from the field of informatin retrieval and use a measurement known as tf-idf, short for term frequency-inverse document frequency. This statistic is intended to reflect how important a word is in a document relative to the other words found in the document, as well as, relative to how often a word may be used across a corpus. \n",
    "\n",
    "#### TfidfVectorizer will not only produce our tf-idf word statistics but it will also first remove less important words known as stopwords and tokenize our corpus. The end result is a sparse matrix where each row represents a document in our corpus and each column is a distinct word found across all of the documents. \n",
    "\n",
    "#### 1) You should be able to understand why we want this information represented as a sparse matrix. Why do you think? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Note that max-df is set such that we keep words that can appear across 100% of the corpus and min-df is set\n",
    "# such that we throw away words that are in less than 10% of our document corpus. We will also generate \n",
    "# unigrams and bigrams which is about right form clinical notes. Beyond bigrams we get very sparse....\n",
    "tfidfVectorizer = TfidfVectorizer(max_df=1.0, max_features=200000,\n",
    "                                  min_df=0.1, stop_words='english',\n",
    "                                  use_idf=True, tokenizer=tokenize, ngram_range=(1,2))   \n",
    "\n",
    "tfidfMatrix = tfidfVectorizer.fit_transform(cleanedCorpus)\n",
    "\n",
    "print(\"**** STORED AS A SPARSE MATRIX ****\")\n",
    "print(tfidfMatrix.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generated Data to Keep\n",
    "\n",
    "#### We want to hold onto the terms that were selected by our tf-idf vectorizer. \n",
    "\n",
    "#### We will also perform some high-dimensional geometry by calculating the cosine angel between multi-dimensional vectors that represent the text documents. Each element in the document vector represents a distint word that was used in the document. This word element is represented as a tf-idf numeric value. The cosine angle between two document vectors can be measured to determine how alike in topic two documents may be. We will calculate the pairwise similarity between all document pairs. We will need these distance measures for later when we try to visualize our clusters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distinctTerms = tfidfVectorizer.get_feature_names()\n",
    "print(\"**** SOME DISTINCT TERMS ***\")\n",
    "print(distinctTerms[-10:])\n",
    "print(distinctTerms[:10])\n",
    "print()\n",
    "dist = 1 - cosine_similarity(tfidfMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Time to Build Clusters\n",
    "\n",
    "#### We will use K-means clustering to search for symantic patterns within our corpus.  K-means is a vector quantization method that attemps to partition $N$ observations (our clinical documents) into $K$ clusters. It assigns an observation to a cluster by trying to allign the observations with the cluster centroid. The cluster centroid represents the center of the cluster. K-means tries to minimize the within-cluster-variance which is a measure of compactness. So the idea is to generate as compact clusters as possible given a set of observations to assign.  \n",
    "\n",
    "#### K-means is known as a hard-clustering method because each observations must be assigned to a single cluster. Other clustering methods such as expectation-maximization are known as soft-clustering methods because an observation can belong to multiple clusters with a certain probability. \n",
    "\n",
    "#### With K-means we need to determine the number of clusters we will produce aprioi. There is no magic here, the best approach is performing some trial and error. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numClusters = 10\n",
    "km = KMeans(n_clusters=numClusters)\n",
    "km.fit(tfidfMatrix)\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Basic Cluster Statistics\n",
    "\n",
    "#### Let's look at the number of documents that were assigned to each cluster. We can also take a look at the group mean of the cluster althogh it does not provide a great deal of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clinicalDocuments = { 'labels': labels, 'rank': ranks, 'corpus': cleanedCorpus, 'cluster': clusters }\n",
    "frame = pd.DataFrame(clinicalDocuments, index = [clusters] , columns = ['rank', 'labels', 'corpus', 'cluster'])\n",
    "print(\"**** CLUSTER COUNTS ****\")\n",
    "print(frame['cluster'].value_counts())\n",
    "print()\n",
    "grouped = frame['rank'].groupby(frame['cluster'])\n",
    "print(\"**** CLUSTER MEANS ****\")\n",
    "print(grouped.mean())\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Better Insight into our Clusters\n",
    "\n",
    "#### We will obviously want to understand what documents clustered together and the important terms that were found common among the documents in the cluster. Check out the cluster output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"**** TOP 10 TERMS PER CLUSTER ****\")\n",
    "orderedCentroids = km.cluster_centers_.argsort()[:, ::-1] # get indexes - sort smallest to largest then reverse order, largest to smallest\n",
    "orderedCentroidScores = []\n",
    "for i in range(numClusters):\n",
    "    orderedCentroidScores.append(km.cluster_centers_[i, orderedCentroids[i,:]])  \n",
    "for i in range(numClusters):\n",
    "    print(\"Cluster# %d Words:\" % i, end='')\n",
    "    j = 0\n",
    "    for indice in orderedCentroids[i, :10]:\n",
    "        print(' %s: %.5f' % (distinctTerms[indice].split(' ')[0], orderedCentroidScores[i][j]), end=',')\n",
    "        j += 1\n",
    "    print()\n",
    "    print()\n",
    "    print(\"Cluster# %d Titles:\" % i, end='')\n",
    "    for label in frame.ix[i]['labels'].values.tolist():\n",
    "        print(' %s,' % label, end='')\n",
    "    print()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization\n",
    "\n",
    "#### We have a bit of a problem in that we can only conceptualize what high-dimensional space must look like. We are only able to visualize 3-dimensional space and most of us prefer 2-dimensional visualization. With text clustering we are dealing with 100's of dimensions, one for each vector term we used. Well the good news is that we have figured out how to perform reduction mappiing which projects high-dimensional space onto lower 2-dimensional space for visualizetion. We will use a process known as multidemensional scaling that attemps to preserve the distance between objects when reduced to low dimensional space. This may take a little time so be patient, eventually you will see the clusters in 2-dimensional space.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.switch_backend('agg')\n",
    "# Multidimensional scaling (MDS) is a means of visualizing the level of similarity of individual cases in a dataset. \n",
    "# MDS aims to place each object in n-dimensional space such that the between-object distances are preserved\n",
    "# in the reduction mapping to two-dimensional space.\n",
    "MDS()\n",
    "# \"precomputed\" because we provide a distance matrix\n",
    "# \"random_state\" so the plot is reproducible\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "#set up colors for each clusters\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e', \n",
    "                  5: '#ffcc99', 6: '#33ffcc', 7: '#FFFF33', 8: '#ccccff', 9: '#99ccff'}\n",
    "#set up cluster names for each color\n",
    "cluster_names = {0: 'Cluster 0', 1: 'Cluster 1', 2: 'Cluster 2', 3: 'Cluster 3', 4: 'Cluster 4', \n",
    "                 5: 'Cluster 5', 6: 'Cluster 6', 7: 'Cluster 7', 8: 'Cluster 8', 9: 'Cluster 9'} \n",
    "#result of the MDS along with the cluster numbers and titles\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=labels)) \n",
    "#group by cluster\n",
    "groups = df.groupby('label')\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "ax.margins(0.05)\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')   \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "#add label in x,y position with the label as the document name\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "plt.show() #show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
